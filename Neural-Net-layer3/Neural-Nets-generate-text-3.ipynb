{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets generates text\n",
    "\n",
    "## LSTM with 3 layers\n",
    " \n",
    "This code is heavily modified from https://github.com/benjelloo/RapNet \n",
    " \n",
    " ### Processing Lyrics Dataset\n",
    "\n",
    " dataprep.py\n",
    " \n",
    "Perform data set preprocessing, including printing of text length, processing of punctuation marks, lowercase conversion of text, segmentation of text, and mapping of characters to integer indexes.\n",
    "Used to prepare text data for subsequent neural network model training.\n",
    "\n",
    "###  build a model and set the hyperparameters\n",
    "\n",
    "model.py\n",
    "\n",
    "The main purpose of this code is to define an RNN model, including the model structure and forward propagation operation, and provides the function of initializing the hidden state.\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnn(\n",
      "  (embedding): Embedding(2157, 256)\n",
      "  (lstm): LSTM(256, 500, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=500, out_features=2157, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "Rnn(\n",
      "  (embedding): Embedding(2157, 256)\n",
      "  (lstm): LSTM(256, 500, num_layers=3, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=500, out_features=2157, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "GPU not found, will train on CPU!\n",
      "Epoch 1/10 \t Loss: 5.699097857475281 \t Progress: 3.6101083032490973% \t Time Elapsed: 0.5346979022026062 minutes\n",
      "Epoch 1/10 \t Loss: 5.501242408752441 \t Progress: 7.2202166064981945% \t Time Elapsed: 1.1332733710606893 minutes\n",
      "Epoch 2/10 \t Loss: 5.161659518204166 \t Progress: 13.610108303249097% \t Time Elapsed: 2.272845137119293 minutes\n",
      "Epoch 2/10 \t Loss: 4.7849772262573245 \t Progress: 17.220216606498195% \t Time Elapsed: 3.1383707880973817 minutes\n",
      "Epoch 3/10 \t Loss: 4.340528424850292 \t Progress: 23.610108303249095% \t Time Elapsed: 4.649322402477265 minutes\n",
      "Epoch 3/10 \t Loss: 3.9949229335784913 \t Progress: 27.220216606498195% \t Time Elapsed: 5.494076816240947 minutes\n",
      "Epoch 4/10 \t Loss: 3.6760962831098483 \t Progress: 33.6101083032491% \t Time Elapsed: 7.024131202697754 minutes\n",
      "Epoch 4/10 \t Loss: 3.3858738851547243 \t Progress: 37.2202166064982% \t Time Elapsed: 7.895649870236714 minutes\n",
      "Epoch 5/10 \t Loss: 3.1154883406256553 \t Progress: 43.61010830324909% \t Time Elapsed: 9.441001784801482 minutes\n",
      "Epoch 5/10 \t Loss: 2.9108307147026062 \t Progress: 47.22021660649819% \t Time Elapsed: 10.315802470842998 minutes\n",
      "Epoch 6/10 \t Loss: 2.6372441747094277 \t Progress: 53.61010830324909% \t Time Elapsed: 11.852184351285299 minutes\n",
      "Epoch 6/10 \t Loss: 2.5183318758010866 \t Progress: 57.2202166064982% \t Time Elapsed: 12.729908569653828 minutes\n",
      "Epoch 7/10 \t Loss: 2.2866935036276694 \t Progress: 63.6101083032491% \t Time Elapsed: 14.298708868026733 minutes\n",
      "Epoch 7/10 \t Loss: 2.166167290210724 \t Progress: 67.2202166064982% \t Time Elapsed: 15.18156571785609 minutes\n",
      "Epoch 8/10 \t Loss: 1.9810689734873799 \t Progress: 73.6101083032491% \t Time Elapsed: 16.706709655125938 minutes\n",
      "Epoch 8/10 \t Loss: 1.8688947367668152 \t Progress: 77.2202166064982% \t Time Elapsed: 17.560175851980844 minutes\n",
      "Epoch 9/10 \t Loss: 1.7298477086643715 \t Progress: 83.61010830324909% \t Time Elapsed: 19.02950615088145 minutes\n",
      "Epoch 9/10 \t Loss: 1.6507337272167206 \t Progress: 87.22021660649818% \t Time Elapsed: 19.59823517004649 minutes\n",
      "Epoch 10/10 \t Loss: 1.5193571694153176 \t Progress: 93.61010830324909% \t Time Elapsed: 20.686757751305898 minutes\n",
      "Epoch 10/10 \t Loss: 1.473315838575363 \t Progress: 97.2202166064982% \t Time Elapsed: 21.297370183467866 minutes\n",
      "model successfully trained and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import dataprep\n",
    "import model\n",
    "\n",
    "# Perform a single forward and backward pass\n",
    "def fb_props(rnn,optimizer,criterion,inp,target,hidden,gpu_avail):\n",
    "    rnn.zero_grad()\n",
    "    if gpu_avail:\n",
    "        inp,target = inp.cuda(),target.cuda()\n",
    "    \n",
    "    hidden = tuple([x.data for x in hidden])\n",
    "\n",
    "    output,hidden = rnn.forward(inp,hidden)\n",
    "    loss = criterion(output.squeeze(),target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(),5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(),hidden\n",
    "\n",
    "# Train the model over multiple epochs\n",
    "def training_loop(rnn,batch_size,optimizer,criterion,n_epochs,gpu_avail):\n",
    "    losses=[]\n",
    "    rnn.train()\n",
    "    t_start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = rnn.init_hidden_weights(batch_size,gpu_avail)\n",
    "        for batch, (inputs,targets) in enumerate(train_loader,1):\n",
    "            #ensure it's a full batch before props\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch > n_batches):\n",
    "                break\n",
    "            loss,hidden = fb_props(rnn,optimizer,criterion,inputs,targets,hidden,gpu_avail)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if batch%100==0:\n",
    "                t_end = time.time()\n",
    "                print('Epoch {}/{} \\t Loss: {} \\t Progress: {}% \\t Time Elapsed: {} minutes'.format(\n",
    "                    epoch+1,\n",
    "                    n_epochs,\n",
    "                    np.average(losses),\n",
    "                    (epoch*n_batches+batch)/(n_epochs*n_batches)*100,\n",
    "                    (t_end-t_start)/60))\n",
    "                losses=[]\n",
    "    return rnn\n",
    "\n",
    "# Pre-process and load the data\n",
    "data_dir = '../Troye Sivan_Lyrics.txt'\n",
    "seq_length = 32\n",
    "batch_size = 128\n",
    "\n",
    "v_to_i,i_to_v,text_nums = dataprep.data_processor(data_dir)\n",
    "train_loader = dataprep.data_batcher(text_nums,seq_length,batch_size)\n",
    "\n",
    "\n",
    "# Define and initialize the LSTM model and set hyperparameters\n",
    "hypers = model.HyperParams(len(v_to_i), len(v_to_i), num_layers=3)  # Set num_layers to 3\n",
    "\n",
    "net = model.Rnn(hypers.vocab_size, hypers.output_size, hypers.embedding_dim, hypers.hidden_dim, hypers.num_layers, hypers.dropout)\n",
    "print(net)\n",
    "\n",
    "net = model.Rnn(hypers.vocab_size,hypers.output_size,hypers.embedding_dim,hypers.hidden_dim,hypers.num_layers,hypers.dropout)\n",
    "print(net)\n",
    "\n",
    "#check for a gpu\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available!')\n",
    "    gpu_avail=True\n",
    "    net.cuda()\n",
    "else:\n",
    "    print('GPU not found, will train on CPU!')\n",
    "    gpu_avail=False\n",
    "\n",
    "# An optimizer (Adam optimizer in this case) and a loss function (cross-entropy loss) are defined\n",
    "optimizer = optim.Adam(net.parameters(),lr=hypers.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start the training process\n",
    "trained_model = training_loop(net,batch_size,optimizer,criterion,hypers.epochs,gpu_avail)\n",
    "\n",
    "# save to a file\n",
    "torch.save(trained_model.state_dict(),'../Neural-Net_layer3/trained_model.pt')\n",
    "print('model successfully trained and saved!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dataprep\n",
    "import numpy as np\n",
    "\n",
    "# Load and processe the text data\n",
    "# Creating mappings between words and numerical indices.\n",
    "data_dir = '../Troye Sivan_Lyrics.txt'\n",
    "v_to_i, i_to_v, text_nums = dataprep.data_processor(data_dir)\n",
    "\n",
    "# Initializes the hyperparameters and the trained model\n",
    "hypers = model.HyperParams(len(v_to_i), len(v_to_i), num_layers=3)  # Set num_layers to 3\n",
    "seq_length = 32\n",
    "\n",
    "\n",
    "trained_net = model.Rnn(hypers.vocab_size, hypers.output_size, hypers.embedding_dim, hypers.hidden_dim, hypers.num_layers, hypers.dropout)\n",
    "trained_net.load_state_dict(torch.load('../Neural-Net_layer3/trained_model.pt'))\n",
    "trained_net.eval()\n",
    "\n",
    "# Sets up the initial conditions for text generation, including the desired length of the generated text and the starting word.\n",
    "gen_length = 600\n",
    "start_word = \"my\"\n",
    "start_word_ind = v_to_i[start_word]\n",
    "generated = [start_word]\n",
    "\n",
    "init_seq = np.full((1, seq_length), v_to_i['<newline>'])\n",
    "init_seq[-1][-1] = start_word_ind\n",
    "\n",
    "# Generate new words by looping\n",
    "# In each loop, the next word is selected based on the model's output and probability distribution, and then the input sequence is updated.\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "for x in range(gen_length):\n",
    "    if gpu_avail:\n",
    "        init_seq = torch.LongTensor(init_seq).cuda()\n",
    "    else:\n",
    "        init_seq = torch.LongTensor(init_seq)\n",
    "    \n",
    "    # Modify the way to initialize the hidden state to ensure compatibility with 3 LSTM layers\n",
    "    hidden = trained_net.init_hidden_weights(init_seq.size(0), gpu_avail)\n",
    "    output, _ = trained_net.forward(init_seq, hidden)\n",
    "    probs = F.softmax(output, dim=1).data\n",
    "\n",
    "    top_k = 10\n",
    "    probs, top_inds = probs.topk(top_k)\n",
    "    top_inds = top_inds.numpy().squeeze()\n",
    "    probs =  probs.numpy().squeeze()\n",
    "    chosen_word_ind = np.random.choice(top_inds, p=probs/probs.sum())\n",
    "    generated.append(i_to_v[chosen_word_ind])\n",
    "\n",
    "    init_seq = np.roll(init_seq, -1, 1)\n",
    "    init_seq[-1][-1] = chosen_word_ind\n",
    "\n",
    "# Joins the generated words into a single string and handles punctuation\n",
    "generated = ' '.join(generated)\n",
    "generated = dataprep.punctuation_handler(generated, for_gen=True)\n",
    "\n",
    "try:\n",
    "    f = open(\"../New lyrics/neural_net3_lyrics.txt\", \"a\")\n",
    "except:\n",
    "    f = open(\"../New lyrics/neural_net3_lyrics.txt\", \"w\")\n",
    "f.write(generated)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### postprocess the text\n",
    " \n",
    " This code is heavily modified from: https://github.com/vickyyyyyyy/lyrics-lstm/blob/main/preprocess.py\n",
    "\n",
    " Reference code(better_profanity): https://pypi.org/project/better-profanity/0.1/\n",
    " \n",
    " This code is mainly used to format the content in the text file and perform basic sensitive word filtering to improve the readability of the text and avoid inappropriate content.\n",
    "\n",
    " This code is mainly used to filter basic sensitive words in text files to improve the readability of the text and avoid inappropriate content.\n",
    "\n",
    " Split the text content, break lines when encountering punctuation marks, and correct capitalization and capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from better_profanity import profanity\n",
    "\n",
    "def postprocess(lyrics, censored):\n",
    "    # Capitalize the beginning of sentences\n",
    "    sentence_case = re.compile(r'(?<=[.?!\\n]\\s)(\\w+)|(^\\w+)')\n",
    "    lyrics = sentence_case.sub(lambda match: match.group().capitalize(), lyrics)\n",
    "    # Finds a standalone lowercase 'i' and converts it to uppercase\n",
    "    lyrics = re.sub(fr'\\si[{string.punctuation}|\\s]|\\si$', lambda match: match.group().upper(), lyrics)\n",
    "    # Remove extra spaces around punctuation marks\n",
    "    lyrics = re.sub(fr\" (?=[{string.punctuation}\\n])|(?<=\\n) \", \"\", lyrics)\n",
    "    return profanity.censor(lyrics) if censored else lyrics\n",
    "\n",
    "# Censorship of indecent terms\n",
    "def process_file(file_path, censored=False):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lyrics = file.read()\n",
    "\n",
    "    processed_lyrics = postprocess(lyrics, censored)\n",
    "\n",
    "    with open('../New lyrics/neural_net_lyrics.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_lyrics)\n",
    "\n",
    "file_path = '../New lyrics/neural_net_lyrics.txt'\n",
    "process_file(file_path, censored=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
