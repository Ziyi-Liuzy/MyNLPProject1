{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets generates text\n",
    "\n",
    "## LSTM model with 2 layers\n",
    " \n",
    "This code is heavily modified from https://github.com/benjelloo/RapNet \n",
    " \n",
    " ### Processing Lyrics Dataset\n",
    "\n",
    " dataprep.py\n",
    " \n",
    "Perform data set preprocessing, including printing of text length, processing of punctuation marks, lowercase conversion of text, segmentation of text, and mapping of characters to integer indexes.\n",
    "Used to prepare text data for subsequent neural network model training.\n",
    "\n",
    "###  build a model and set the hyperparameters\n",
    "\n",
    "model.py\n",
    "\n",
    "The main purpose of this code is to define an LSTM model and set the hyperparameters.\n",
    "\n",
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rnn(\n",
      "  (embedding): Embedding(2157, 256)\n",
      "  (lstm): LSTM(256, 500, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (fc): Linear(in_features=500, out_features=2157, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "GPU not found, will train on CPU!\n",
      "Epoch 1/10 \t Loss: 5.636845269203186 \t Progress: 3.6101083032490973% \t Time Elapsed: 0.5745039025942484 minutes\n",
      "Epoch 1/10 \t Loss: 5.036985006332397 \t Progress: 7.2202166064981945% \t Time Elapsed: 1.1743528485298156 minutes\n",
      "Epoch 2/10 \t Loss: 4.5191188195330945 \t Progress: 13.610108303249097% \t Time Elapsed: 2.226372464497884 minutes\n",
      "Epoch 2/10 \t Loss: 4.020382084846497 \t Progress: 17.220216606498195% \t Time Elapsed: 2.828109665711721 minutes\n",
      "Epoch 3/10 \t Loss: 3.536372304636206 \t Progress: 23.610108303249095% \t Time Elapsed: 3.9054867307345074 minutes\n",
      "Epoch 3/10 \t Loss: 3.189838402271271 \t Progress: 27.220216606498195% \t Time Elapsed: 4.5007169008255 minutes\n",
      "Epoch 4/10 \t Loss: 2.806339490211616 \t Progress: 33.6101083032491% \t Time Elapsed: 5.5665975173314415 minutes\n",
      "Epoch 4/10 \t Loss: 2.5318904614448545 \t Progress: 37.2202166064982% \t Time Elapsed: 6.186567199230194 minutes\n",
      "Epoch 5/10 \t Loss: 2.2205168251263894 \t Progress: 43.61010830324909% \t Time Elapsed: 7.268696149190267 minutes\n",
      "Epoch 5/10 \t Loss: 1.9881611812114715 \t Progress: 47.22021660649819% \t Time Elapsed: 7.881462832291921 minutes\n",
      "Epoch 6/10 \t Loss: 1.7716641991825428 \t Progress: 53.61010830324909% \t Time Elapsed: 8.967480818430582 minutes\n",
      "Epoch 6/10 \t Loss: 1.5867651009559631 \t Progress: 57.2202166064982% \t Time Elapsed: 9.578532485167186 minutes\n",
      "Epoch 7/10 \t Loss: 1.4204714968379608 \t Progress: 63.6101083032491% \t Time Elapsed: 10.66280799706777 minutes\n",
      "Epoch 7/10 \t Loss: 1.2946812850236893 \t Progress: 67.2202166064982% \t Time Elapsed: 11.266973666350047 minutes\n",
      "Epoch 8/10 \t Loss: 1.1291124487327318 \t Progress: 73.6101083032491% \t Time Elapsed: 12.383883066972096 minutes\n",
      "Epoch 8/10 \t Loss: 1.0367800045013427 \t Progress: 77.2202166064982% \t Time Elapsed: 13.00601813395818 minutes\n",
      "Epoch 9/10 \t Loss: 0.9210188961298452 \t Progress: 83.61010830324909% \t Time Elapsed: 14.076418900489807 minutes\n",
      "Epoch 9/10 \t Loss: 0.8513718140125275 \t Progress: 87.22021660649818% \t Time Elapsed: 14.683864665031432 minutes\n",
      "Epoch 10/10 \t Loss: 0.7494074346992255 \t Progress: 93.61010830324909% \t Time Elapsed: 15.750807734330495 minutes\n",
      "Epoch 10/10 \t Loss: 0.6756422087550163 \t Progress: 97.2202166064982% \t Time Elapsed: 16.354458431402843 minutes\n",
      "model successfully trained and saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "import dataprep\n",
    "import model\n",
    "\n",
    "# Perform a single forward and backward pass\n",
    "def fb_props(rnn,optimizer,criterion,inp,target,hidden,gpu_avail):\n",
    "    rnn.zero_grad()\n",
    "    if gpu_avail:\n",
    "        inp,target = inp.cuda(),target.cuda()\n",
    "    \n",
    "    hidden = tuple([x.data for x in hidden])\n",
    "\n",
    "    output,hidden = rnn.forward(inp,hidden)\n",
    "    loss = criterion(output.squeeze(),target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(),5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(),hidden\n",
    "\n",
    "# Train the model over multiple epochs\n",
    "def training_loop(rnn,batch_size,optimizer,criterion,n_epochs,gpu_avail):\n",
    "    losses=[]\n",
    "    rnn.train()\n",
    "    t_start = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        hidden = rnn.init_hidden_weights(batch_size,gpu_avail)\n",
    "        for batch, (inputs,targets) in enumerate(train_loader,1):\n",
    "            #ensure it's a full batch before props\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch > n_batches):\n",
    "                break\n",
    "            loss,hidden = fb_props(rnn,optimizer,criterion,inputs,targets,hidden,gpu_avail)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            if batch%100==0:\n",
    "                t_end = time.time()\n",
    "                print('Epoch {}/{} \\t Loss: {} \\t Progress: {}% \\t Time Elapsed: {} minutes'.format(\n",
    "                    epoch+1,\n",
    "                    n_epochs,\n",
    "                    np.average(losses),\n",
    "                    (epoch*n_batches+batch)/(n_epochs*n_batches)*100,\n",
    "                    (t_end-t_start)/60))\n",
    "                losses=[]\n",
    "    return rnn\n",
    "\n",
    "# Pre-process and load the data\n",
    "data_dir = '../Troye Sivan_Lyrics.txt'\n",
    "seq_length = 32\n",
    "batch_size = 128\n",
    "\n",
    "v_to_i,i_to_v,text_nums = dataprep.data_processor(data_dir)\n",
    "train_loader = dataprep.data_batcher(text_nums,seq_length,batch_size)\n",
    "\n",
    "# Define and initialize the LSTM model and set hyperparameters\n",
    "hypers = model.HyperParams(len(v_to_i),len(v_to_i))\n",
    "\n",
    "net = model.Rnn(hypers.vocab_size,hypers.output_size,hypers.embedding_dim,hypers.hidden_dim,hypers.num_layers,hypers.dropout)\n",
    "print(net)\n",
    "\n",
    "#check for a gpu\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available!')\n",
    "    gpu_avail=True\n",
    "    net.cuda()\n",
    "else:\n",
    "    print('GPU not found, will train on CPU!')\n",
    "    gpu_avail=False\n",
    "\n",
    "# An optimizer (Adam optimizer in this case) and a loss function (cross-entropy loss) are defined\n",
    "optimizer = optim.Adam(net.parameters(),lr=hypers.learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Start the training process\n",
    "trained_model = training_loop(net,batch_size,optimizer,criterion,hypers.epochs,gpu_avail)\n",
    "\n",
    "# save to a file\n",
    "torch.save(trained_model.state_dict(),'trained_model.pt')\n",
    "print('model successfully trained and saved!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import dataprep\n",
    "import numpy as np\n",
    "\n",
    "# Load and processe the text data\n",
    "# Creating mappings between words and numerical indices.\n",
    "data_dir = '../Troye Sivan_Lyrics.txt'\n",
    "v_to_i,i_to_v,text_nums = dataprep.data_processor(data_dir)\n",
    "\n",
    "# Initializes the hyperparameters and the trained model\n",
    "hypers = model.HyperParams(len(v_to_i),len(v_to_i))\n",
    "seq_length = 32\n",
    "\n",
    "trained_net = model.Rnn(hypers.vocab_size,hypers.output_size,hypers.embedding_dim,hypers.hidden_dim,hypers.num_layers,hypers.dropout)\n",
    "trained_net.load_state_dict(torch.load('trained_model.pt'))\n",
    "trained_net.eval()\n",
    "\n",
    "# Sets up the initial conditions for text generation, including the desired length of the generated text and the starting word.\n",
    "gen_length = 600\n",
    "start_word = \"my\"\n",
    "start_word_ind = v_to_i[start_word]\n",
    "generated = [start_word]\n",
    "\n",
    "init_seq = np.full((1,seq_length),v_to_i['<newline>'])\n",
    "init_seq[-1][-1] = start_word_ind\n",
    "\n",
    "# Generate new words by looping\n",
    "# In each loop, the next word is selected based on the model's output and probability distribution, and then the input sequence is updated.\n",
    "gpu_avail = torch.cuda.is_available()\n",
    "for x in range(gen_length):\n",
    "    if gpu_avail:\n",
    "        init_seq = torch.LongTensor(init_seq).cuda()\n",
    "    else:\n",
    "        init_seq = torch.LongTensor(init_seq)\n",
    "    hidden = trained_net.init_hidden_weights(init_seq.size(0),gpu_avail)\n",
    "    output, _ = trained_net.forward(init_seq,hidden)\n",
    "    probs = F.softmax(output,dim=1).data\n",
    "\n",
    "    top_k = 10\n",
    "    probs,top_inds = probs.topk(top_k)\n",
    "    top_inds = top_inds.numpy().squeeze()\n",
    "    probs =  probs.numpy().squeeze()\n",
    "    chosen_word_ind = np.random.choice(top_inds,p=probs/probs.sum())\n",
    "    generated.append(i_to_v[chosen_word_ind])\n",
    "\n",
    "    init_seq = np.roll(init_seq,-1,1)\n",
    "    init_seq[-1][-1] = chosen_word_ind\n",
    "\n",
    "# Joins the generated words into a single string and handles punctuation\n",
    "generated = ' '.join(generated)\n",
    "generated = dataprep.punctuation_handler(generated,for_gen=True)\n",
    "\n",
    "try:\n",
    "    f = open(\"../New lyrics/neural_net_lyrics.txt\",\"a\")\n",
    "except:\n",
    "    f = open(\"../New lyrics/neural_net_lyrics.txt\",\"w\")\n",
    "f.write(generated)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Postprocess the text\n",
    " \n",
    " This code is heavily modified from: https://github.com/vickyyyyyyy/lyrics-lstm/blob/main/preprocess.py\n",
    "\n",
    " Reference code(better_profanity): https://pypi.org/project/better-profanity/0.1/\n",
    " \n",
    " This code is mainly used to format the content in the text file and perform basic sensitive word filtering to improve the readability of the text and avoid inappropriate content.\n",
    "\n",
    " This code is mainly used to filter basic sensitive words in text files to improve the readability of the text and avoid inappropriate content.\n",
    "\n",
    " Split the text content, break lines when encountering punctuation marks, and correct capitalization and capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from better_profanity import profanity\n",
    "\n",
    "def postprocess(lyrics, censored):\n",
    "    # Capitalize the beginning of sentences\n",
    "    sentence_case = re.compile(r'(?<=[.?!\\n]\\s)(\\w+)|(^\\w+)')\n",
    "    lyrics = sentence_case.sub(lambda match: match.group().capitalize(), lyrics)\n",
    "    # Finds a standalone lowercase 'i' and converts it to uppercase\n",
    "    lyrics = re.sub(fr'\\si[{string.punctuation}|\\s]|\\si$', lambda match: match.group().upper(), lyrics)\n",
    "    # Remove extra spaces around punctuation marks\n",
    "    lyrics = re.sub(fr\" (?=[{string.punctuation}\\n])|(?<=\\n) \", \"\", lyrics)\n",
    "    return profanity.censor(lyrics) if censored else lyrics\n",
    "\n",
    "# Censorship of indecent terms\n",
    "def process_file(file_path, censored=False):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lyrics = file.read()\n",
    "\n",
    "    processed_lyrics = postprocess(lyrics, censored)\n",
    "\n",
    "    with open('../New lyrics/neural_net_lyrics.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_lyrics)\n",
    "\n",
    "file_path = '../New lyrics/neural_net_lyrics.txt'\n",
    "process_file(file_path, censored=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
